"""empty message

Revision ID: f205bd2f321e
Revises: 48df2056a7c5
Create Date: 2025-04-21 05:48:13.781938

"""
from alembic import op
import sqlalchemy as sa
import sqlalchemy_utils
import sqlmodel # added
import pgvector.sqlalchemy
from sqlalchemy import text


# revision identifiers, used by Alembic.
revision = 'f205bd2f321e'
down_revision = '48df2056a7c5'
branch_labels = None
depends_on = None


def upgrade():
    op.execute("CREATE EXTENSION IF NOT EXISTS pg_trgm") 
    op.execute("CREATE EXTENSION IF NOT EXISTS vector")
    
    # Check if the enum types already exist and create them only if they don't
    conn = op.get_bind()
    
    # Check for documenttypeenum
    result = conn.execute(text("SELECT EXISTS (SELECT 1 FROM pg_type WHERE typname = 'documenttypeenum')")).scalar()
    if not result:
        # Create the enum type if it doesn't exist
        op.execute(text("CREATE TYPE documenttypeenum AS ENUM ('Image', 'Audio', 'Video')"))
    
    # Check for documentprocessingstatusenum
    result = conn.execute(text("SELECT EXISTS (SELECT 1 FROM pg_type WHERE typname = 'documentprocessingstatusenum')")).scalar()
    if not result:
        # Create the enum type if it doesn't exist
        op.execute(text("CREATE TYPE documentprocessingstatusenum AS ENUM ('Queued', 'Processing', 'Extracting', 'ExtractionCompleted', 'AnalysisCompleted', 'Success', 'Failed', 'Exception')"))
    
    # Check for chunktypeenum
    result = conn.execute(text("SELECT EXISTS (SELECT 1 FROM pg_type WHERE typname = 'chunktypeenum')")).scalar()
    if not result:
        # Create the enum type if it doesn't exist
        op.execute(text("CREATE TYPE chunktypeenum AS ENUM ('ImageDescription' , 'ImageText', 'ImageObject', 'AudioSegment', 'Speaker', 'VideoScene')"))
    
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('documents',
    sa.Column('id', sqlmodel.sql.sqltypes.GUID(), nullable=False),
    sa.Column('updated_at', sa.DateTime(), nullable=True),
    sa.Column('created_at', sa.DateTime(), nullable=True),
    sa.Column('deleted_at', sa.DateTime(), nullable=True),
    sa.Column('file_id', sqlmodel.sql.sqltypes.GUID(), nullable=False),
    sa.Column('dataset_id', sqlmodel.sql.sqltypes.GUID(), nullable=False),
    # sa.Column('document_type', sa.Enum('Image', 'Audio', 'Video', name='documenttypeenum'), nullable=False),
    # sa.Column('processing_status', sa.Enum('Queued', 'Processing', 'Extracting', 'ExtractionCompleted', 'AnalysisCompleted', 'Success', 'Failed', 'Exception', name='documentprocessingstatusenum'), nullable=False),
    sa.Column('description', sqlmodel.sql.sqltypes.AutoString(), nullable=True),
    sa.Column('description_embedding', pgvector.sqlalchemy.vector.VECTOR(dim=3072), nullable=True),
    sa.Column('task_id', sqlmodel.sql.sqltypes.AutoString(), nullable=True),
    sa.Column('error_message', sqlmodel.sql.sqltypes.AutoString(), nullable=True),
    sa.Column('file_path', sqlmodel.sql.sqltypes.AutoString(), nullable=False),
    sa.Column('file_size', sa.Integer(), nullable=True),
    sa.Column('mime_type', sqlmodel.sql.sqltypes.AutoString(), nullable=True),
    sa.Column('document_metadata', sa.JSON(), nullable=True),
    sa.Column('ingestion_id', sqlmodel.sql.sqltypes.GUID(), nullable=True),
    sa.Column('processed_at', sa.DateTime(), nullable=True),
    sa.ForeignKeyConstraint(['dataset_id'], ['datasets.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_documents_dataset_id'), 'documents', ['dataset_id'], unique=False)
    op.create_index(op.f('ix_documents_file_id'), 'documents', ['file_id'], unique=False)
    op.create_index(op.f('ix_documents_id'), 'documents', ['id'], unique=False)
    # Add the document_type column separately using direct SQL to avoid automatic type creation
    op.execute(
        text("ALTER TABLE documents ADD COLUMN document_type documenttypeenum NOT NULL")
    ),

    # Add the processing_status column separately using direct SQL to avoid automatic type creation
    op.execute(
        text("ALTER TABLE documents ADD COLUMN processing_status documentprocessingstatusenum NOT NULL")
    ),
    op.create_table('document_chunks',
    sa.Column('id', sqlmodel.sql.sqltypes.GUID(), nullable=False),
    sa.Column('updated_at', sa.DateTime(), nullable=True),
    sa.Column('created_at', sa.DateTime(), nullable=True),
    sa.Column('deleted_at', sa.DateTime(), nullable=True),
    sa.Column('document_id', sqlmodel.sql.sqltypes.GUID(), nullable=False),
    # sa.Column('chunk_type', sa.Enum('ImageText', 'ImageObject', 'AudioSegment', 'Speaker', 'VideoScene', name='chunktypeenum'), nullable=False),
    sa.Column('chunk_text', sqlmodel.sql.sqltypes.AutoString(), nullable=False),
    sa.Column('chunk_embedding', pgvector.sqlalchemy.vector.VECTOR(dim=3072), nullable=True),
    sa.Column('chunk_metadata', sa.JSON(), nullable=True),
    sa.ForeignKeyConstraint(['document_id'], ['documents.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_document_chunks_id'), 'document_chunks', ['id'], unique=False)
    # Add the document_chunks column separately using direct SQL to avoid automatic type creation
    op.execute(
        text("ALTER TABLE document_chunks ADD COLUMN chunk_type chunktypeenum NOT NULL")
    ),
    # ### end Alembic commands ###


def downgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_index(op.f('ix_document_chunks_id'), table_name='document_chunks')
    op.drop_table('document_chunks')
    op.drop_index(op.f('ix_documents_id'), table_name='documents')
    op.drop_index(op.f('ix_documents_file_id'), table_name='documents')
    op.drop_index(op.f('ix_documents_dataset_id'), table_name='documents')
    op.drop_table('documents')
    # ### end Alembic commands ###
